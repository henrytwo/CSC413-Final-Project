\documentclass[12pt]{article}
\usepackage{listings}
\usepackage{setspace}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{amsmath,amsthm,amssymb}
\usepackage[margin=2.5cm]{geometry}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{fancyhdr}
\usepackage{hyperref}

% Math stuff
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

% Image stuff
\graphicspath{ {.} }

% URL Stuff
\hypersetup{
colorlinks=true,
linkcolor=blue,
filecolor=magenta,
urlcolor=cyan,
}

% Header/Foodter setup
\pagestyle{fancy}
\fancyhf{}
\rhead{CSC413 Final Project}
\lhead{Henry Tu and Seel Patel}
\rfoot{Page \thepage}
\lfoot{April 18, 2021}

% Code style setup
\lstdefinestyle{Python}{
language = Python,
frame = lines,
numbers = left,
basicstyle = \scriptsize,
keywordstyle = \color{blue},
stringstyle = \color{green},
commentstyle = \color{red}\ttfamily
}

% To be honest, I have no idea what this is used for
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
\hskip -\arraycolsep
\let\@ifnextchar\new@ifnextchar
\array{#1}}
\makeatother
\onehalfspacing

% Some not sketchy latex
\begin{document}
    \begin{center}
        \textbf{Comparing the interpretability of different GAN architectures}
    \end{center}
    \begin{minipage}{.5\textwidth}
        \centering
        \textbf{Henry Tu}\\
        University of Toronto
    \end{minipage}
    \begin{minipage}{.5\textwidth}
        \centering
        \textbf{Seel Patel}\\
        University of Toronto
    \end{minipage}
    \\

    \begin{multicols*}{2}
        \raggedcolumns

        % Document starts here
        \section{Abstract}
        \label{sec:abstract}
        A Generative Adversarial Network (GAN) uses an adversarial process between two models which are simultaneously trained to estimate a generative model.\cite{gan}
        There are many variants of GAN architectures, such as COCO-GAN\cite{cocogan} and StyleGAN\cite{stylegan}, which both have the ability to generate synthetic images which mimic real images.\\\\
        We are exploring methods of comparing the quantitative performance of these architectures with their interpretability.
        Our quantitative measures include C2ST\cite{evaluateGANs}, image quality measures\cite{evaluateGANs}, Maximum Mean Discrepancy (MMD)\cite{evaluateGANs}, etc.\\\\
        In order to analyze these networks qualitatively, we will use methods such as SmoothGrad\cite{smoothgrad}, Latent Space Exploration\cite{sampleGAN} and nearest neighbour tests\cite{evaluateGANs} to decipher what the networks have learned.\\\\
        By combining these two forms of analysis, we hope to gain insight into the relationship between performance metrics and the generated output of the models.

        \section{Introduction}
        \label{sec:introduction}
        StyleGAN and COCO-GAN generate images using different techniques to accomplish two different goals: StyleGAN is designed such that high level features of the output image can be finely tuned and adjusted (e.g. lighting, hair colour, etc.)\cite{stylegan}.
        On the other hand, COCO-GAN generates each part of the image separately before stitching it together in order to simulate the human perception of vision\cite{cocogan}.
        As a result, we expected each model to generate images with different qualities that relate to their designed tasks.\\\\
        Although we tried to analyze the models using all the metrics listed in the Abstract, there were a few challenges involved with getting the desired results.

        \section{Analysis Dataset Generation}
        \label{sec:dataset}
        Both GAN models were trained on the LSUN Bedroom dataset\cite{lsunBedroom} at a resolution of $256 \times 256$.
        Due to time constraints, we used pre-trained models to generate images for our analysis.

        \label{sec:datasetGeneration}
        \subsection{StyleGAN}
        \label{subsec:styleganGeneration}
        NVIDIA Research Projects' Official TensorFlow Implementation of StyleGAN pretrained to the LSUN Bedroom dataset\cite{styleGANCode} was used to generate 5000 images for analysis.
        5000 latent code vectors $\mathbf{z} \in \mathbb{R}^{512}$ drawn from a standard normal distribution which each correspond to an output image.


        \subsection{COCO-GAN}
        \label{subsec:cocoganGeneration}

        \section{Quantitative Analysis}
        \label{sec:quantitative}
        \subsection{C2ST}
        \label{subsec:c2st}
        Classifier Two-Sample Tests (C2ST) is used to predict if two samples came from the same distribution\cite{c2st}.
        In the context of evaluating GANs, we will use C2ST to quantitatively measure how well the models mimic real images.
        We trained a deep convolutional neural network to perform binary classification.
        The idea is to compare the margin by which each GAN model is able to trick the classifier to determine how realistic the image is.
        \\\\
        To avoid introducing bias, all three datasets (training, validation, test) were equally balanced in real and fake images.
        The 5000 fake images were further divided in half between images from StyleGAN and COCO-GAN .
        Out of the combined pool of 10000 images, 5000 were allocated for testing, 2500 validation, and 2500 testing.
        \\\\
        The classifier was implemented using Pytorch with the following network architecture:
        \\[insert some stuff here]\\
        After tuning hyper parameters and performing early stopping with the validation dataset, the following accuracies were attained:
        \begin{verbatim}Training Accuracy:
Validation Accuracy:
Test Accuracy (Real):
Test Accuracy (StyleGAN):
Test Accuracy (COCO-GAN):\end{verbatim}
        As seen here, the model failed to accurately discriminate between the real and generated images.
        This suggests that the image quality

        \subsection{Image Quality}
        \label{subsec:imageQuality}
        asd
        \subsection{MMD}
        \label{subsec:mmd}
        asd

        \section{Qualitative Analysis}
        \label{sec:qualitative}
        \subsection{SmoothGrad}
        \label{subsec:smoothgrad}
        asd
        \subsection{Latent Space Exploration}
        \label{subsec:latentSpaceExploration}
        asd
        \subsection{Nearest Neighbours}
        \label{subsec:nearestneighbours}
        asd
    \end{multicols*}
    \newpage
    \begin{thebibliography}{9}
        \bibitem{gan}
        Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. (2014)
        \href{https://papers.nips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf}{\textit{Generative Adversarial Networks} }

        \bibitem{cocogan}
        Chieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, Hwann-Tzong Chen. (2019)
        \href{https://arxiv.org/pdf/1904.00284.pdf}{\textit{COCO-GAN: Generation by Parts via Conditional Coordinating} }

        \bibitem{stylegan}
        Tero Karras, Samuli Laine, Timo Aila. (2018)
        \href{https://arxiv.org/pdf/1812.04948.pdf}{\textit{A Style-Based Generator Architecture for Generative Adversarial Networks} }

        \bibitem{evaluateGANs}
        Brownlee, Jason. (2019)
        \href{https://machinelearningmastery.com/how-to-evaluate-generative-adversarial-networks/}{\textit{How to Evaluate Generative Adversarial Networks.} }

        \bibitem{smoothgrad}
        Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Viegas, Martin Wattenberg. (2017)
        \href{https://arxiv.org/pdf/1706.03825.pdf}{\textit{SmoothGrad: removing noise by adding noise} }

        \bibitem{sampleGAN}
        Tom White. (2016)
        \href{https://arxiv.org/pdf/1609.04468.pdf}{\textit{Sampling Generative Networks} }

        \bibitem{lsunBedroom}
        Yu, Fisher and Zhang, Yinda and Song, Shuran and Seff, Ari and Xiao, Jianxiong. (2015)
        \href{https://arxiv.org/pdf/1506.03365.pdf}{\textit{LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop}}

        \bibitem{styleGANCode}
        NVLabs. (2019) \href{https://github.com/NVlabs/stylegan}{\textit{StyleGAN â€” Official TensorFlow Implementation}}

        \bibitem{c2st}
        David Lopez-Paz, Maxime Oquab. (2016)
        \href{https://research.fb.com/wp-content/uploads/2017/04/neural_tests.pdf?}{\textit{Revisiting Classifier Two-Sample Tests for GAN Evaluation and Causal Discovery}}

    \end{thebibliography}

\end{document}